{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-IMDB-Anup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPBqB5lbeg2f8R3hmyjetU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anupkkumarfb/LSTM/blob/master/LSTM_IMDB_Anup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBaOXrrZNMOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cca156ae-2eb8-444e-8074-c23889eb3c60"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, LSTM\n",
        "\n",
        "\n",
        "np_load_old = np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a,allow_pickle=True)\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "np.load = np_load_old\n",
        "\n",
        "\n",
        "print (\"The first review is : {}, its length is :{}, its type is: {}\".format(X_train[1],len(X_train[1]), type(X_train)))\n",
        "\n",
        "#Padding to make sure all reviews are of equal length\n",
        "max_review_length = 500\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "#print (\"The first review is : {}, shape is: {}\".format(X_train[1],X_train.shape))\n",
        "\n",
        "embedding_vector_length = 32  \n",
        "#Embedding layer - given any indices returns a vector. can be thought of word2vec.  Embedding_vector_length = number of activation functions\n",
        "\n",
        "#Sequential model --> Output of one layer goes as input to the next\n",
        "model = Sequential()\n",
        "\n",
        "#Adding an embedding layer\n",
        "model.add(Embedding(top_words+1, embedding_vector_length, input_length=max_review_length))\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words+1, embedding_vector_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))  #Each LSTM cell will take 32 outputs from previous layer (activation function) as input\n",
        "#model.add(Dense(1,activation='sigmoid'))\n",
        "model.add(Dense(100,activation='sigmoid'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "epochs = 10\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=epochs)\n",
        "print (\"Time to train the model with {} epochs: {}\".format(epochs,(time.time()-start)))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print (\"Time to evaluate the model with {}\".format((time.time()-start)))\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "print(\"Scores: \", scores)\n",
        "\n",
        "\n",
        "#For predicting output\n",
        "\n",
        "print(\"New review: \\'the movie was a great waste of my time\\'\")\n",
        "d = imdb.get_word_index()\n",
        "#print(d)\n",
        "review = \"Awesome movie. Excellent. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. Amazing. \"\n",
        "\n",
        "words = review.split()\n",
        "review = []\n",
        "for word in words:\n",
        "  if word not in d: \n",
        "    print(\"Word: \", word)\n",
        "    review.append(2)\n",
        "  else:\n",
        "    print(\"Word in else: \", word)\n",
        "    review.append(d[word]+3) \n",
        "    print(d[word])\n",
        "\n",
        "print(review)\n",
        "\n",
        "review = sequence.pad_sequences([review],truncating='pre', padding='pre', maxlen=max_review_length)\n",
        "prediction = model.predict(review)\n",
        "print(\"Prediction (0 = negative, 1 = positive) = \", end=\"\")\n",
        "print(\"%0.4f%%\" % (prediction[0][0]*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first review is : [1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95], its length is :189, its type is: <class 'numpy.ndarray'>\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 500, 32)           160032    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 223,433\n",
            "Trainable params: 223,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.4777 - accuracy: 0.7616\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.3169 - accuracy: 0.8705\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.2524 - accuracy: 0.8991\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.2185 - accuracy: 0.9154\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.1916 - accuracy: 0.9254\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.1589 - accuracy: 0.9403\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.1343 - accuracy: 0.9522\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.1257 - accuracy: 0.9539\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.0996 - accuracy: 0.9657\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.1298 - accuracy: 0.9543\n",
            "Time to train the model with 10 epochs: 192.9431324005127\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.4673 - accuracy: 0.8264\n",
            "Time to evaluate the model with 7.979522943496704\n",
            "Accuracy: 82.64%\n",
            "Scores:  [0.4672675132751465, 0.8264399766921997]\n",
            "New review: 'the movie was a great waste of my time'\n",
            "Word:  Awesome\n",
            "Word:  movie.\n",
            "Word:  Excellent.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "Word:  Amazing.\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Prediction (0 = negative, 1 = positive) = 73.0481%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dedybz4SOES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "00abc8a5-5ce3-4d00-b6fd-e72fa01b8d88"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New review: 'the movie was a great waste of my time'\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "Word:  Awesome\n",
            "Word:  movie.\n",
            "Word:  Excellent.\n",
            "Word:  Amazing\n",
            "[2, 2, 2, 2]\n",
            "Prediction (0 = negative, 1 = positive) = 21.2508%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}